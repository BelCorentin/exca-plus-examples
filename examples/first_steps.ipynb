{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "Understand and use exca in a very basic scenario: simply to create a simple model, config and cache it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Philosophy\n",
    "\n",
    "#### Pure Python\n",
    "\n",
    "The tools here do not provide a script API but a way to do everything directly from Python. Specific script APIs can be easily composed on top of it if needed.\n",
    "\n",
    "#### Parameter Validation\n",
    "\n",
    "Configurations should be validated before running to avoid discovering bugs later (e.g., missing parameters, inconsistent parameters, wrong types, etc.). We achieve this by using `pydantic.BaseModel`, which works like dataclasses but validates all parameters.\n",
    "\n",
    "#### Fast Configs\n",
    "\n",
    "Running a grid search requires creating numerous configurations, so they should be easy and fast to create. This means not deferring the loading of data, PyTorch models, etc., to later.\n",
    "\n",
    "#### No Parameter Duplication - Easy to Extend\n",
    "\n",
    "Configurations hold the underlying actual function/class parameters. To avoid duplicating parameters, we couple configs with the actual classes/functions:\n",
    "\n",
    "```python\n",
    "class MyClassCfg(pydantic.BaseModel):\n",
    "    x: int = 12\n",
    "    y: str = \"hello\"\n",
    "\n",
    "    def build(self) -> \"MyClass\":\n",
    "        return MyClass(self)\n",
    "\n",
    "class MyClass:\n",
    "    def __init__(self, cfg: MyClassCfg):\n",
    "        self.cfg = cfg\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first example\n",
    "\n",
    "Validating configurations and understand why it's important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before anything:\n",
    "```\n",
    "pip install exca torch pydantic yaml typing pathlib sklearn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydantic\n",
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "Let's create a simple model configuration class using pydantic.\n",
    "This class will define the model's hyperparameters and provide a method to build the model.\n",
    "\"\"\"\n",
    "class ConvCfg(pydantic.BaseModel):\n",
    "    layers: int = 12\n",
    "    kernel: int = 5\n",
    "    channels: int = 128\n",
    "\n",
    "    def build(self) -> torch.nn.Module:\n",
    "        # instantiate when needed\n",
    "        # (do not slow down config initialization)\n",
    "        return ConvModel(self)  \n",
    "\n",
    "\n",
    "class ConvModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, cfg: ConvCfg) -> None:\n",
    "        self.cfg = cfg\n",
    "\n",
    "# then in your code\n",
    "model = ConvCfg().build()  # Works ! :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, if we want to create a yaml config, with a parameter that's of a wrong type, it doesn't work! And even before sending it to slurm!\n",
    "config = \"\"\"\n",
    "model:\n",
    "  layers: 12\n",
    "  kernel: 5\n",
    "  channels: \"hi\"\n",
    "\"\"\"\n",
    "import yaml\n",
    "config = yaml.safe_load(config)\n",
    "model = ConvCfg(**config['model']).build()  # Raises an error! :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminated unions (one step further)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydantic\n",
    "import torch\n",
    "from typing import Any, Dict, Optional\n",
    "from torch import nn\n",
    "import typing as tp\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from pydantic import BaseModel, Field\n",
    "from exca import TaskInfra, MapInfra\n",
    "    \n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, cfg: \"TransformerCfg\") -> None:\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        # define your transformer model here\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # define the forward pass\n",
    "        pass\n",
    "class ConvCfg(pydantic.BaseModel):\n",
    "    name: tp.Literal[\"conv\"] = \"conv\"  # special discriminator field\n",
    "    layers: int = 12\n",
    "    kernel: int = 5\n",
    "    channels: int = 128\n",
    "\n",
    "    def build(self) -> torch.nn.Module:\n",
    "        return ConvModel(self) #instantiate when needed\n",
    "\n",
    "...\n",
    "\n",
    "class TransformerCfg(pydantic.BaseModel):\n",
    "    model_config = pydantic.ConfigDict(extra=\"forbid\")  # pydantic boilerplate: safer\n",
    "    name: tp.Literal[\"transformer\"] = \"transformer\"  # special discriminator field\n",
    "    layers: int = 12\n",
    "    embeddings: int = 128\n",
    "\n",
    "    def build(self) -> torch.nn.Module:\n",
    "        return TransformerModel(self)\n",
    "\n",
    "...\n",
    "\n",
    "class Trainer(pydantic.BaseModel):\n",
    "    model: ConvCfg | TransformerCfg = pydantic.Field(..., discriminator=\"name\")\n",
    "    optimizer: str = \"Adam\"\n",
    "    infra: TaskInfra = TaskInfra()\n",
    "\n",
    "    @infra.apply\n",
    "    def run(self) -> float:\n",
    "        model = self.model.build()  # build either one of the model\n",
    "        # specific location for this very config:\n",
    "        ckpt_path = self.infra.uid_folder() / \"checkpoint.pt\"\n",
    "        if ckpt_path.exists():\n",
    "           # load\n",
    "           ...\n",
    "        ...\n",
    "        # for batch in loader:\n",
    "        #     ...\n",
    "        # return accuracy\n",
    "        return 100.\n",
    "\n",
    "\n",
    "string = \"\"\"\n",
    "model:\n",
    "  name: transformer  # specifies which model\n",
    "  embeddings: 256  # only accepts transformer specific parameters\n",
    "optimizer: SGD\n",
    "\"\"\"\n",
    "trainer = Trainer(**yaml.safe_load(string))\n",
    "\n",
    "isinstance(trainer.model, TransformerCfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do it with a ConvCfg too\n",
    "string = \"\"\"\n",
    "model:\n",
    "  name: conv  # specifies which model\n",
    "  layers: 12\n",
    "  kernel: 5\n",
    "  channels: 128\n",
    "optimizer: Adam\n",
    "\"\"\"\n",
    "trainer = Trainer(**yaml.safe_load(string))\n",
    "isinstance(trainer.model, ConvCfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to instantiate the objects anymore / import them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do it on a slurm cluster\n",
    "\n",
    "Let's try to play with the most important part: launching jobs on a cluster, and caching the results.\n",
    "\n",
    "Works as well in local!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "tmp_path = Path(\"/tmp\")\n",
    "string = f\"\"\"\n",
    "model:\n",
    "  name: transformer  # specifies which model\n",
    "  embeddings: 256\n",
    "optimizer: SGD\n",
    "infra:\n",
    "  gpus_per_node: 8\n",
    "  cpus_per_task: 80\n",
    "  slurm_constraint: volta32gb\n",
    "  folder: {tmp_path}\n",
    "  cluster: auto\n",
    "  slurm_partition: learnfair\n",
    "  workdir:\n",
    "    copied:\n",
    "      - . # copies current working directory into a dedicated workdir\n",
    "      # - whatever_other_file_or_folder\n",
    "\"\"\"\n",
    "\n",
    "trainer = Trainer(**yaml.safe_load(string))\n",
    "with trainer.infra.job_array() as array:\n",
    "    for layers in [12, 14, 15]:\n",
    "        array.append(trainer.infra.clone_obj({\"model.layers\": layers}))\n",
    "# leaving the context submits all trainings in a job array\n",
    "# and is non-blocking\n",
    "\n",
    "# show one of the slurm jobs\n",
    "print(array[0].infra.job())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rst = array[0].run()  # run the first job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /tmp/__main__.Trainer.run,0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are those folders? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add hierarchical classes, and \"\"\"real life\"\"\" example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A minimalist example with sklearn to show how to develop and explore a model with exca.\n",
    "\"\"\"\n",
    "import typing as tp\n",
    "import numpy as np\n",
    "import pydantic\n",
    "import sys\n",
    "import exca\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "class Dataset(pydantic.BaseModel):\n",
    "    n_samples: int = 100\n",
    "    noise: float = 0.1\n",
    "    random_state: int = 42\n",
    "    test_size: float = 0.2\n",
    "    model_config = pydantic.ConfigDict(extra=\"forbid\")\n",
    "\n",
    "    def get(self) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        # Generate synthetic data\n",
    "        X, y = make_regression(\n",
    "            n_samples=self.n_samples,\n",
    "            noise=self.noise,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        # Split into training and testing datasets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, \n",
    "            test_size=self.test_size, \n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "class Model(pydantic.BaseModel):\n",
    "    data: Dataset = Dataset()\n",
    "    alpha: float = 1.0\n",
    "    max_iter: int = 1000\n",
    "    infra: exca.TaskInfra = exca.TaskInfra(folder='.cache/', version='v1.0')\n",
    "\n",
    "    @infra.apply\n",
    "    def score(self):\n",
    "        # Get data\n",
    "        X_train, X_test, y_train, y_test = self.data.get()\n",
    "\n",
    "        # Train a Ridge regression model\n",
    "        print('Fit...')\n",
    "        model = Ridge(alpha=self.alpha, max_iter=self.max_iter)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate\n",
    "        print('Score...')\n",
    "        y_pred = model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        return mse\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Validate config\n",
    "    basic_config = {\"alpha\": 1.0, \"max_iter\": 1000}\n",
    "    config = exca.ConfDict(basic_config)\n",
    "    model = Model(**config)\n",
    "    print(model.infra.config)\n",
    "\n",
    "    # Score\n",
    "    mse = model.score()\n",
    "    print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls .cache/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the scoring function: make it a new version !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(pydantic.BaseModel):\n",
    "    data: Dataset = Dataset()\n",
    "    alpha: float = 1.0\n",
    "    max_iter: int = 1000\n",
    "    infra: exca.TaskInfra = exca.TaskInfra(folder='.cache/', version='v2.0')\n",
    "\n",
    "    @infra.apply\n",
    "    def score(self):\n",
    "        # Get data\n",
    "        X_train, X_test, y_train, y_test = self.data.get()\n",
    "\n",
    "        # Train a Ridge regression model\n",
    "        print('Fit...')\n",
    "        # model = Ridge(alpha=self.alpha, max_iter=self.max_iter)\n",
    "\n",
    "        ## NEW VERSION: use not a Ridge model but a LinearRegression model\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate\n",
    "        print('Score...')\n",
    "        y_pred = model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        return mse\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Validate config\n",
    "    basic_config = {\"alpha\": 1.0, \"max_iter\": 1000}\n",
    "    config = exca.ConfDict(basic_config)\n",
    "    model = Model(**config)\n",
    "    print(model.infra.config)\n",
    "\n",
    "    # Score\n",
    "    mse = model.score()\n",
    "    print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls .cache/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other important topics\n",
    "\n",
    "### MapInfra\n",
    "\n",
    "If you want to iterate on your tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
