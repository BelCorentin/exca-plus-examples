{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "Understand and use exca in a very basic scenario: simply to create a simple model, config and cache it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Philosophy\n",
    "\n",
    "#### Pure Python\n",
    "\n",
    "The tools here do not provide a script API but a way to do everything directly from Python. Specific script APIs can be easily composed on top of it if needed.\n",
    "\n",
    "#### Parameter Validation\n",
    "\n",
    "Configurations should be validated before running to avoid discovering bugs later (e.g., missing parameters, inconsistent parameters, wrong types, etc.). We achieve this by using `pydantic.BaseModel`, which works like dataclasses but validates all parameters.\n",
    "\n",
    "#### Fast Configs\n",
    "\n",
    "Running a grid search requires creating numerous configurations, so they should be easy and fast to create. This means not deferring the loading of data, PyTorch models, etc., to later.\n",
    "\n",
    "#### No Parameter Duplication - Easy to Extend\n",
    "\n",
    "Configurations hold the underlying actual function/class parameters. To avoid duplicating parameters, we couple configs with the actual classes/functions:\n",
    "\n",
    "```python\n",
    "class MyClassCfg(pydantic.BaseModel):\n",
    "    x: int = 12\n",
    "    y: str = \"hello\"\n",
    "\n",
    "    def build(self) -> \"MyClass\":\n",
    "        return MyClass(self)\n",
    "\n",
    "class MyClass:\n",
    "    def __init__(self, cfg: MyClassCfg):\n",
    "        self.cfg = cfg\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first example\n",
    "\n",
    "Validating configurations and understand why it's important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before anything:\n",
    "```\n",
    "pip install exca torch pydantic yaml typing pathlib sklearn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The base for config validations (before sending anything on a cluster / loading data / computing stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydantic\n",
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "Let's create a simple model configuration class using pydantic.\n",
    "This class will define the model's hyperparameters and provide a method to build the model.\n",
    "\"\"\"\n",
    "class ConvCfg(pydantic.BaseModel):\n",
    "    layers: int = 12\n",
    "    kernel: int = 5\n",
    "    channels: int = 128\n",
    "    necessary_param: str\n",
    "\n",
    "    def build(self) -> torch.nn.Module:\n",
    "        # instantiate when needed\n",
    "        # (do not slow down config initialization)\n",
    "        return ConvModel(self)  \n",
    "\n",
    "\n",
    "class ConvModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, cfg: ConvCfg) -> None:\n",
    "        self.cfg = cfg\n",
    "\n",
    "# then in your code\n",
    "model = ConvCfg(necessary_param='str').build()  # Works ! :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for ConvCfg\nchannels\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='hi', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\nnecessary_param\n  Field required [type=missing, input_value={'layers': 12, 'kernel': 5, 'channels': 'hi'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myaml\u001b[39;00m\n\u001b[32m      9\u001b[39m config = yaml.safe_load(config)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m model = \u001b[43mConvCfg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.build()  \u001b[38;5;66;03m# Raises an error! :(\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/neuralset/lib/python3.12/site-packages/pydantic/main.py:243\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    242\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    245\u001b[39m     warnings.warn(\n\u001b[32m    246\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    247\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    249\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    250\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 2 validation errors for ConvCfg\nchannels\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='hi', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\nnecessary_param\n  Field required [type=missing, input_value={'layers': 12, 'kernel': 5, 'channels': 'hi'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing"
     ]
    }
   ],
   "source": [
    "# Now, if we want to create a yaml config, with a parameter that's of a wrong type, it doesn't work! And even before sending it to slurm!\n",
    "config = \"\"\"\n",
    "model:\n",
    "  layers: 12\n",
    "  kernel: 5\n",
    "  channels: \"hi\"\n",
    "\"\"\"\n",
    "import yaml\n",
    "config = yaml.safe_load(config)\n",
    "model = ConvCfg(**config['model']).build()  # Raises an error! :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Very simple and basic exca example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exca import TaskInfra\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class random_mock_class(pydantic.BaseModel):\n",
    "    infra: TaskInfra = TaskInfra(folder='test')\n",
    "\n",
    "    @infra.apply()\n",
    "    def function_i_dont_fancy_recomputing(self):\n",
    "        # very hard computation\n",
    "        # Display some progress bar over a few seconds\n",
    "\n",
    "        for i in tqdm(range(10)):\n",
    "            # do some computation\n",
    "            import time\n",
    "            time.sleep(0.1)\n",
    "        return 0+0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random = random_mock_class()\n",
    "random.function_i_dont_fancy_recomputing()  # Works! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mconfig.yaml\u001b[0m*  \u001b[01;32mfull-uid.yaml\u001b[0m*  \u001b[01;32mjob.pkl\u001b[0m*  \u001b[01;32muid.yaml\u001b[0m*\n"
     ]
    }
   ],
   "source": [
    "ls test/__main__.random_mock_class.function_i_dont_fancy_recomputing,0/default/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminated unions (one step further)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydantic\n",
    "import torch\n",
    "from typing import Any, Dict, Optional\n",
    "from torch import nn\n",
    "import typing as tp\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from pydantic import BaseModel, Field\n",
    "from exca import TaskInfra, MapInfra\n",
    "    \n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, cfg: \"TransformerCfg\") -> None:\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        # define your transformer model here\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # define the forward pass\n",
    "        pass\n",
    "class ConvCfg(pydantic.BaseModel):\n",
    "    name: tp.Literal[\"conv\"] = \"conv\"  # special discriminator field\n",
    "    layers: int = 12\n",
    "    kernel: int = 5\n",
    "    channels: int = 128\n",
    "\n",
    "    def build(self) -> torch.nn.Module:\n",
    "        return ConvModel(self) #instantiate when needed\n",
    "\n",
    "...\n",
    "\n",
    "class TransformerCfg(pydantic.BaseModel):\n",
    "    model_config = pydantic.ConfigDict(extra=\"forbid\")  # pydantic boilerplate: safer\n",
    "    name: tp.Literal[\"transformer\"] = \"transformer\"  # special discriminator field\n",
    "    layers: int = 12\n",
    "    embeddings: int = 128\n",
    "\n",
    "    def build(self) -> torch.nn.Module:\n",
    "        return TransformerModel(self)\n",
    "\n",
    "...\n",
    "\n",
    "class Trainer(pydantic.BaseModel):\n",
    "    model: ConvCfg | TransformerCfg = pydantic.Field(..., discriminator=\"name\")\n",
    "    optimizer: str = \"Adam\"\n",
    "    infra: TaskInfra = TaskInfra()\n",
    "\n",
    "    @infra.apply\n",
    "    def run(self) -> float:\n",
    "        model = self.model.build()  # build either one of the model\n",
    "        # specific location for this very config:\n",
    "        ckpt_path = self.infra.uid_folder() / \"checkpoint.pt\"\n",
    "        if ckpt_path.exists():\n",
    "           # load\n",
    "           ...\n",
    "        ...\n",
    "        # for batch in loader:\n",
    "        #     ...\n",
    "        # return accuracy\n",
    "        return 100.\n",
    "\n",
    "\n",
    "string = \"\"\"\n",
    "model:\n",
    "  name: transformer  # specifies which model\n",
    "  embeddings: 256  # only accepts transformer specific parameters\n",
    "optimizer: SGD\n",
    "\"\"\"\n",
    "trainer = Trainer(**yaml.safe_load(string))\n",
    "\n",
    "isinstance(trainer.model, TransformerCfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do it with a ConvCfg too\n",
    "string = \"\"\"\n",
    "model:\n",
    "  name: conv  # specifies which model\n",
    "  layers: 12\n",
    "  kernel: 5\n",
    "  channels: 128\n",
    "optimizer: Adam\n",
    "\"\"\"\n",
    "trainer = Trainer(**yaml.safe_load(string))\n",
    "isinstance(trainer.model, ConvCfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to instantiate the objects anymore / import them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do it on a slurm cluster\n",
    "\n",
    "Let's try to play with the most important part: launching jobs on a cluster, and caching the results.\n",
    "\n",
    "Works as well in local!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "tmp_path = Path(\"/tmp\")\n",
    "string = f\"\"\"\n",
    "model:\n",
    "  name: transformer  # specifies which model\n",
    "  embeddings: 256\n",
    "optimizer: SGD\n",
    "infra:\n",
    "  gpus_per_node: 8\n",
    "  cpus_per_task: 80\n",
    "  slurm_constraint: volta32gb\n",
    "  folder: {tmp_path}\n",
    "  cluster: auto\n",
    "  slurm_partition: learnfair\n",
    "  workdir:\n",
    "    copied:\n",
    "      - . # copies current working directory into a dedicated workdir\n",
    "      # - whatever_other_file_or_folder\n",
    "\"\"\"\n",
    "\n",
    "trainer = Trainer(**yaml.safe_load(string))\n",
    "with trainer.infra.job_array() as array:\n",
    "    for layers in [12, 14, 15]:\n",
    "        array.append(trainer.infra.clone_obj({\"model.layers\": layers}))\n",
    "# leaving the context submits all trainings in a job array\n",
    "# and is non-blocking\n",
    "\n",
    "# show one of the slurm jobs\n",
    "print(array[0].infra.job())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rst = array[0].run()  # run the first job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /tmp/__main__.Trainer.run,0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are those folders? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add hierarchical classes, and \"\"\"real life\"\"\" example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A minimalist example with sklearn to show how to develop and explore a model with exca.\n",
    "\"\"\"\n",
    "import typing as tp\n",
    "import numpy as np\n",
    "import pydantic\n",
    "import sys\n",
    "import exca\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "class Dataset(pydantic.BaseModel):\n",
    "    n_samples: int = 100\n",
    "    noise: float = 0.1\n",
    "    random_state: int = 42\n",
    "    test_size: float = 0.2\n",
    "    model_config = pydantic.ConfigDict(extra=\"forbid\")\n",
    "\n",
    "    def get(self) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        # Generate synthetic data\n",
    "        X, y = make_regression(\n",
    "            n_samples=self.n_samples,\n",
    "            noise=self.noise,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        # Split into training and testing datasets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, \n",
    "            test_size=self.test_size, \n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "class Model(pydantic.BaseModel):\n",
    "    data: Dataset = Dataset()\n",
    "    alpha: float = 1.0\n",
    "    max_iter: int = 1000\n",
    "    infra: exca.TaskInfra = exca.TaskInfra(folder='.cache/', version='v1.0')\n",
    "\n",
    "    @infra.apply\n",
    "    def score(self):\n",
    "        # Get data\n",
    "        X_train, X_test, y_train, y_test = self.data.get()\n",
    "\n",
    "        # Train a Ridge regression model\n",
    "        print('Fit...')\n",
    "        model = Ridge(alpha=self.alpha, max_iter=self.max_iter)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate\n",
    "        print('Score...')\n",
    "        y_pred = model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        return mse\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Validate config\n",
    "    basic_config = {\"alpha\": 1.0, \"max_iter\": 1000}\n",
    "    config = exca.ConfDict(basic_config)\n",
    "    model = Model(**config)\n",
    "    print(model.infra.config)\n",
    "\n",
    "    # Score\n",
    "    mse = model.score()\n",
    "    print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls .cache/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the scoring function: make it a new version !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(pydantic.BaseModel):\n",
    "    data: Dataset = Dataset()\n",
    "    alpha: float = 1.0\n",
    "    max_iter: int = 1000\n",
    "    infra: exca.TaskInfra = exca.TaskInfra(folder='.cache/', version='v2.0')\n",
    "\n",
    "    @infra.apply\n",
    "    def score(self):\n",
    "        # Get data\n",
    "        X_train, X_test, y_train, y_test = self.data.get()\n",
    "\n",
    "        # Train a Ridge regression model\n",
    "        print('Fit...')\n",
    "        # model = Ridge(alpha=self.alpha, max_iter=self.max_iter)\n",
    "\n",
    "        ## NEW VERSION: use not a Ridge model but a LinearRegression model\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate\n",
    "        print('Score...')\n",
    "        y_pred = model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        return mse\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Validate config\n",
    "    basic_config = {\"alpha\": 1.0, \"max_iter\": 1000}\n",
    "    config = exca.ConfDict(basic_config)\n",
    "    model = Model(**config)\n",
    "    print(model.infra.config)\n",
    "\n",
    "    # Score\n",
    "    mse = model.score()\n",
    "    print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls .cache/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other important topics\n",
    "\n",
    "### MapInfra\n",
    "\n",
    "If you want to iterate on your tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
